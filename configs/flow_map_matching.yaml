data:
  dataset: "celeba"
  root: /scr/kartiksh/Diffusion_flow_matching/data/celeba-subset
  from_hub: false
  repo_name: "electronickale/cmu-10799-celeba64-subset"
  image_size: 64
  channels: 3
  num_workers: 4
  pin_memory: true
  augment: true

model:
  base_channels: 128
  channel_mult: [1, 2, 2, 4]
  num_res_blocks: 2
  attention_resolutions: [16, 8]
  num_heads: 4
  dropout: 0.0
  use_scale_shift_norm: true

training:
  batch_size: 32
  learning_rate: 0.0002
  weight_decay: 0.0000
  betas: [0.9, 0.999]
  ema_decay: 0.999
  ema_start: 1000
  gradient_clip_norm: 0.5 # 1.0
  num_iterations: 250000  # Flow Map Matching may need more iterations than Flow Matching
  log_every: 100
  sample_every: 1000
  save_every: 5000
  num_samples: 32
  lr_drop_step: 30000   # same as warmup_steps
  lr_drop_factor: 0.3

# Flow Map Matching specific config (Prop. 3.11 from paper)
flow_map_matching:
  num_timesteps: 1000           # Number of discretization steps for sampling
  sigma_min: 0.0001             # Small value to avoid numerical issues at boundaries
  annealing_schedule: "linear"  # Schedule for diagonal annealing: "linear", "cosine", or "none"
  initial_gap: 0.1              # Initial maximum |t-s| during training (curriculum learning)
  warmup_steps: 30000           # Steps to anneal from initial_gap to 1.0
  reconstruction_weight: 0.5    # Weight for reconstruction term in Prop. 3.11 (w_{s,t} = 1)
  # --- New extensions ---
  semigroup_weight: 0.2         # Weight for semigroup consistency loss; 0.0 to disable
  predict_x0: false              # true = pMF x̂ reparameterization; false = original v_θ prediction
  clamp_x: false                # Whether to clamp x during sampling for stability (can be removed if not needed)
  step_epsilon: 1.0e-3          # Denominator ε; sign-safe: forward=(t-s)+ε, backward=(t-s)-ε. Increase to 5e-3 if noisy.
  sg_ramp_steps: 50000   # ramp semigroup after warmup
  sg_min_seg: 0.05       # ignore tiny (t-s) or (u-t) segments for sg loss
  sg_delay_steps: 20000 # delay semigroup loss for some steps after warmup to allow initial learning of v_θ

sampling:
  num_steps: 50  # Sequential composition steps (can use fewer than training timesteps)
  batch_size: 16

infrastructure:
  seed: 42
  device: "cuda"
  num_gpus: 2
  mixed_precision: false  # Note: AMP + torch.func (vmap/jvp) can be unstable in some PyTorch versions
                          # If you encounter NaN/inf losses, try setting this to false
  compile_model: false

checkpoint:
  dir: "./checkpoints"
  resume: null

logging:
  dir: "./logs"
  wandb:
    enabled: true
    project: "cmu-10799-diffusion"
    entity: null
